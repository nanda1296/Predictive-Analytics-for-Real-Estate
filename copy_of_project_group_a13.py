# -*- coding: utf-8 -*-
"""Copy of Project_Group_A13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NToIts91kBw7sxLWhjoA5uHmUlUNNapC
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.impute import KNNImputer

df = pd.read_excel('/content/Spring 2023 - AISC - Nest Analytics_Toronto_housing_data_Cleaned.xlsx')
df = df.drop(df.columns[0],axis=1)
df.head()

df.shape

#checking the data types of all the features
df.info()

#Checking for null values of each feature
df.isnull().sum().sort_values(ascending=False)

#Checking for unique rows in each feature
df.nunique().sort_values(ascending=False)

df.describe()

"""#Data Exploration"""

plt.figure(figsize=[12,6])
sns.heatmap(df.corr(), annot=True, cmap="Reds")
plt.title('Correlation Matrix \n'.center(10))
plt.show()

df = df.drop(['Property Address','Suite/ RESIDENTIAL CONDO#','Legal Reference','Sale Date','Owner Name','Address'], axis=1)

col = list(df.columns)
categorical_features = []
numerical_features = []

for i in col:
        if df[i].dtype == 'object' or df[i].dtype == 'bool':
            categorical_features.append(i)
        else:
            numerical_features.append(i)

print('Categorical_Features: ', categorical_features)
print('Numerical_Features: ', numerical_features)

print('\nInference: The Dataset has {} categorical & {} numerical features.'.format(len(categorical_features),len(numerical_features)))

df[categorical_features] = df[categorical_features].fillna(df[categorical_features].mode().iloc[0])

imputer = KNNImputer(n_neighbors=3)
imputed = imputer.fit_transform(df[numerical_features])
imputed

data_imputed = pd.DataFrame(imputed, columns=numerical_features)
data_imputed.head()

df2 = df.drop(numerical_features, axis=1)
df = pd.concat([df2, data_imputed], axis=1)
df.head()

df.shape

"""The Datset consists of 19 features and 56636 records after filling the null values"""

#Checking for null values of each feature after data cleaning
df.isnull().sum()

"""#Exploratory Data Analysis (EDA)"""

sns.set(style='whitegrid')
f,ax = plt.subplots(2,2,figsize = (16,12))

vis1 = sns.distplot(df['Sale Price'], bins=40, color='red', kde=True, hist_kws=dict(edgecolor='black', linewidth=2),ax= ax[0][0])
vis2 = sns.distplot(df['Land Value'], bins=40, color='red', kde=True, hist_kws=dict(edgecolor='black', linewidth=2),ax= ax[0][1])
vis3 = sns.distplot(df['Building Value'], bins=40, color='red', kde=True, hist_kws=dict(edgecolor='black', linewidth=2),ax= ax[1][0])
vis4 = sns.distplot(df['Total Value'], bins=40, color='red', kde=True, hist_kws=dict(edgecolor='black', linewidth=2),ax= ax[1][1])

plt.show()

def scatter_df():
  for feature in numerical_features:
    if feature != 'Sale Price':
      plot = sns.scatterplot(x=df[feature], y=df['Sale Price'])
      plt.title('{} / Sale Price'.format(feature), fontsize = 16)
      plt.show()

scatter_df()

df['Land Use'].value_counts()

df['Tax District'].value_counts()

df['Foundation Type'].value_counts()

df['Exterior Wall'].value_counts()

df['Grade'].value_counts()

#One-hot encoding the categorical columns

from sklearn.preprocessing import OneHotEncoder

OH_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore', min_frequency=500)
OH_cols = pd.DataFrame(OH_encoder.fit_transform(df[categorical_features]))
OH_cols.index = df.index
df3 = df.drop(categorical_features, axis=1)
df = pd.concat([df3, OH_cols], axis=1)

df.shape

df.columns

"""#Feature Selection"""

target= 'Sale Price'
X = df.drop([target], axis=1)
y = df[target]

X.columns = X.columns.astype(str)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

print('Original set --> ', X.shape,y.shape, '\nTraining set --> ', X_train.shape,y_train.shape, '\nTesting set  --> ', X_test.shape,y_test.shape)

"""#Modeling"""

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error

from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from xgboost import XGBRegressor

# Scaling the numerical features using
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.select_dtypes(['int','float']))
X_test_scaled = scaler.transform(X_test.select_dtypes(['int', 'float']))

#Linear Regression
model1 = LinearRegression()
lr = model1.fit(X_train_scaled, y_train)

print("LR Train R2 Score: ", lr.score(X_train_scaled, y_train))
print("LR Test R2 Score: ", lr.score(X_test_scaled, y_test))

y_pred_lr = model1.predict(X_test_scaled)

mae = mean_absolute_error(y_test, y_pred_lr)
print('Linear Regression MAE: ', mae)

poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train_scaled)
X_test_poly = poly.transform(X_test_scaled)

X_train_poly.shape

X_test_poly.shape

polymodel = LinearRegression()
pf = polymodel.fit(X_train_poly, y_train)

print("PLR Train R2 Score: ", pf.score(X_train_poly, y_train))
print("PLR Test R2 Score: ", pf.score(X_test_poly, y_test))

ridge = Ridge(alpha=1)
rd = ridge.fit(X_train_poly, y_train)

print("RidgeReg Train R2 Score: ", rd.score(X_train_poly, y_train))
print("RidgeReg Test R2 Score: ", rd.score(X_test_poly, y_test))

y_pred_rd = ridge.predict(X_test_poly)

mae_rd = mean_absolute_error(y_test, y_pred_rd)
print('Ridge Regression MAE: ', mae_rd)

lasso = Lasso(alpha=1)
ls = lasso.fit(X_train_poly, y_train)

print("Lasso Train R2 Score: ", ls.score(X_train_poly, y_train))
print("Lasso Test R2 Score: ", ls.score(X_test_poly, y_test))

y_pred_ls = lasso.predict(X_test_poly)

mae_ls = mean_absolute_error(y_test, y_pred_ls)
print('Lasso Regression MAE: ', mae_ls)

gbr = GradientBoostingRegressor(n_estimators=2500, learning_rate=0.1, max_depth=2)
gbr.fit(X_train_scaled, y_train)

print("GB Train R2 Score: ", gbr.score(X_train_scaled, y_train))
print("GB Test R2 Score: ", gbr.score(X_test_scaled, y_test))

xgb = XGBRegressor(n_estimators=2500, learning_rate=0.5, n_jobs=5, reg_alpha=5)
xgb.fit(X_train_scaled, y_train)

print("XGB R^2 Score: ", xgb.score(X_train_scaled, y_train))
print("XGB Test R^2 Score: ", xgb.score(X_test_scaled, y_test))

import pickle

# Commented out IPython magic to ensure Python compatibility.
# %%writefile save_xgb.py
# # Define the filename for saving the model
# filename = 'xgb.pkl'
# 
# # Save the model to the file using pickle
# with open(filename, 'wb') as file:
#     pickle.dump(xgb, file)
#